[{"authors":["admin"],"categories":null,"content":"I am a research scientist and lecturer in the domain of cognitive, neuro-inspired AI. I have always been fascinated by the brain and how its complex electrochemical circuitry gives rise to cognition and behavior. Providing a meaningful contribution to our understanding of how the brain computes the mind is thus one of my most ambitious life goals and a pursuit I feel very passionate about. My research aims to unravel fundamental computational principles and mechanisms of neural information processing (with a particular emphasis on the mammalian neocortex) and to exploit them to develop adaptive intelligent systems. I am particularly interested in sequential rule learning and the acquisition and representation of temporal structure across different scales in modular, hierarchical systems.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://rcfduarte.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a research scientist and lecturer in the domain of cognitive, neuro-inspired AI. I have always been fascinated by the brain and how its complex electrochemical circuitry gives rise to cognition and behavior.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://rcfduarte.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Agnes Korcsak-Gorzo","Charl Linssen","Jasper Albers","Stefan Dasbach","Renato Duarte","Susanne Kunkel","Abigail Morrison","Johanna Senk","Jonas Stapmanns","Tom Tetzlaff","Markus Diesmann","Sacha J. van Albada"],"categories":null,"content":"","date":1670976000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670976000,"objectID":"253604fdbd27dfaf407765f3e3c56698","permalink":"https://rcfduarte.github.io/publication/springer_neuromethods/","publishdate":"2022-12-15T00:00:00Z","relpermalink":"/publication/springer_neuromethods/","section":"publication","summary":"This chapter sheds light on the synaptic organization of the brain from the perspective of computational neuroscience. It provides an introductory overview on how to account for empirical data in mathematical models, implement them in software, and perform simulations reflecting experiments. This path is demonstrated with respect to four key aspects of synaptic signaling: the connectivity of brain networks, synaptic transmission, synaptic plasticity, and the heterogeneity across synapses. Each step and aspect of the modeling and simulation workflow comes with its own challenges and pitfalls, which are highlighted and addressed in detail.","tags":null,"title":"Phenomenological modeling of diverse and heterogeneous synaptic dynamics at natural density","type":"publication"},{"authors":["Tobias Schulte to Brinke","Renato Duarte","Abigail Morrison"],"categories":null,"content":"","date":1664755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664755200,"objectID":"834d854f0f3ebfca554be97a7ad4d67d","permalink":"https://rcfduarte.github.io/publication/brinke_2022_column/","publishdate":"2022-11-04T00:00:00Z","relpermalink":"/publication/brinke_2022_column/","section":"publication","summary":"The neocortex, and with it the mammalian brain, achieves a level of computational efficiency like no other existing computational engine. A deeper understanding of its building blocks (cortical microcircuits), and their underlying computational principles is thus of paramount interest. To this end, we need reproducible computational models that can be analyzed, modified, extended and quantitatively compared. In this study, we further that aim by providing a replication of a seminal cortical column model. This model consists of noisy Hodgkin-Huxley neurons connected by dynamic synapses, whose connectivity scheme is based on empirical findings from intracellular recordings. Our analysis confirms the key original finding that the specific, data-based connectivity structure enhances the computational performance compared to a variety of alternatively structured control circuits. For this comparison, we use tasks based on spike patterns and rates that require the systems not only to have simple classification capabilities, but also to retain information over time and to be able to compute nonlinear functions. Going beyond the scope of the original study, we demonstrate that this finding is independent of the complexity of the neuron model, which further strengthens the argument that it is the connectivity which is crucial. Finally, a detailed analysis of the memory capabilities of the circuits reveals a stereotypical memory profile common across all circuit variants. Notably, the circuit with laminar structure does not retain stimulus any longer than any other circuit type. We therefore conclude that the model's computational advantage lies in a sharper representation of the stimuli.","tags":null,"title":"Characteristic columnar connectivity caters to cortical computation: Replication, simulation, and evaluation of a microcircuit model","type":"publication"},{"authors":["Alessio Quaresima","Hartmut Fitz","Renato Duarte","Dick van den Broek","Peter Hagoort","Karl Magnus Petersson"],"categories":null,"content":"","date":1664323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664323200,"objectID":"e908cf49cf3b7d9929ba268d69b218b1","permalink":"https://rcfduarte.github.io/publication/tripod_2022/","publishdate":"2022-11-04T00:00:00Z","relpermalink":"/publication/tripod_2022/","section":"publication","summary":"Neuron models with explicit dendritic dynamics have shed light on mechanisms for coincidence detection, pathway selection and temporal filtering. However, it is still unclear which morphological and physiological features are required to capture these phenomena. In this work, we introduce the Tripod neuron model and propose a minimal structural reduction of the dendritic tree that is able to reproduce these computations. The Tripod is a three-compartment model consisting of two segregated passive dendrites and a somatic compartment modelled as an adaptive, exponential integrate-and-fire neuron. It incorporates dendritic geometry, membrane physiology and receptor dynamics as measured in human pyramidal cells. We characterize the response of the Tripod to glutamatergic and GABAergic inputs and identify parameters that support supra-linear integration, coincidence-detection and pathway-specific gating through shunting inhibition. Following NMDA spikes, the Tripod neuron generates plateau potentials whose duration depends on the dendritic length and the strength of synaptic input. When fitted with distal compartments, the Tripod encodes previous activity into a dendritic depolarized state. This dendritic memory allows the neuron to perform temporal binding, and we show that it solves transition and sequence detection tasks on which a single-compartment model fails. Thus, the Tripod can account for dendritic computations previously explained only with more detailed neuron models or neural networks. Due to its simplicity, the Tripod neuron can be used efficiently in simulations of larger cortical circuits.","tags":["Neuron","Dendrites","Synapses"],"title":"The Tripod neuron: a minimal structural reduction of the dendritic tree","type":"publication"},{"authors":["Barna Zajzon","David Dahmenn","Abigail Morrison","Renato Duarte"],"categories":null,"content":"","date":1641945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641945600,"objectID":"c82cb67aec225d46e7870cd54067a2c1","permalink":"https://rcfduarte.github.io/publication/signal_denoising/","publishdate":"2022-01-19T00:00:00Z","relpermalink":"/publication/signal_denoising/","section":"publication","summary":"Information from the sensory periphery is conveyed to the cortex via structured projection pathways that spatially segregate stimulus features, providing a robust and efficient encoding strategy. Beyond sensory encoding, this prominent anatomical feature extends throughout the neocortex. However, the extent to which it influences cortical processing is unclear. In this study, we combine cortical circuit modeling with network theory to demonstrate that the sharpness of topographic projections acts as a bifurcation parameter, controlling the macroscopic dynamics and representational precision across a modular network. By shifting the balance of excitation and inhibition, topographic modularity gradually increases task performance and improves the signal-to-noise ratio across the system. We show that this is a robust and generic structural feature that enables a broad range of behaviorally-relevant operating regimes, and provide an in-depth theoretical analysis unravelling the dynamical principles underlying the mechanism.","tags":null,"title":"Signal denoising through topographic modularity of neural circuits","type":"publication"},{"authors":["","吳恩達"],"categories":["Demo","教程"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://rcfduarte.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Claudia Bachmann","Tom Tetzlaff","Renato Duarte","Abigail Morrison"],"categories":null,"content":"","date":1598313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598313600,"objectID":"2309fb6d3fb7cfe34e7c1088e6804c39","permalink":"https://rcfduarte.github.io/publication/ad_paper/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/publication/ad_paper/","section":"publication","summary":"The impairment of cognitive function in Alzheimer's is clearly correlated to synapse loss. However, the mechanisms underlying this correlation are only poorly understood. Here, we investigate how the loss of excitatory synapses in sparsely connected random networks of spiking excitatory and inhibitory neurons alters their dynamical characteristics. Beyond the effects on the network's activity statistics, we find that the loss of excitatory synapses on excitatory neurons shifts the network dynamic towards the stable regime. The decrease in sensitivity to small perturbations to time varying input can be considered as an indication of a reduction of computational capacity. A full recovery of the network performance can be achieved by firing rate homeostasis, here implemented by an up-scaling of the remaining excitatory-excitatory synapses. By analysing the stability of the linearized network dynamics, we explain how homeostasis can simultaneously maintain the network's firing rate and sensitivity to small perturbations.","tags":null,"title":"Firing rate homeostasis counteracts changes in stability of recurrent neural networks caused by synapse loss in Alzheimer's disease","type":"publication"},{"authors":["Hartmut Fitz","Marvin Uhlmann","Dick van den Broek","Renato Duarte","Peter Hagoort","Karl Magnus Petersson"],"categories":null,"content":"","date":1598313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598313600,"objectID":"cab3607d9221ffe357d69d660ba9de92","permalink":"https://rcfduarte.github.io/publication/neuronal_memory_2020/","publishdate":"2019-02-11T00:00:00Z","relpermalink":"/publication/neuronal_memory_2020/","section":"publication","summary":"Language processing involves the ability to store and integrate pieces of information in working memory over short periods of time. According to the dominant view, information is maintained through sustained, elevated neural activity. Other work has argued that short-term synaptic facilitation can serve as a substrate of memory. Here we propose an account where memory is supported by intrinsic plasticity that downregulates neuronal firing rates. Single neuron responses are dependent on experience, and we show through simulations that these adaptive changes in excitability provide memory on timescales ranging from milliseconds to seconds. On this account, spiking activity writes information into coupled dynamic variables that control adaptation and move at slower timescales than the membrane potential. From these variables, information is continuously read back into the active membrane state for processing. This neuronal memory mechanism does not rely on persistent activity, excitatory feedback, or synaptic plasticity for storage. Instead, information is maintained in adaptive conductances that reduce firing rates and can be accessed directly without cued retrieval. Memory span is systematically related to both the time constant of adaptation and baseline levels of neuronal excitability. Interference effects within memory arise when adaptation is long lasting. We demonstrate that this mechanism is sensitive to context and serial order which makes it suitable for temporal integration in sequence processing within the language domain. We also show that it enables the binding of linguistic features over time within dynamic memory registers. This work provides a step toward a computational neurobiology of language.","tags":null,"title":"Neuronal spike-rate adaptation supports working memory in language processing","type":"publication"},{"authors":["Philipp Weidel","Renato Duarte","Abigail Morrison"],"categories":null,"content":"","date":1584403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584403200,"objectID":"0674381a47979a11b6b9a7a6a6447822","permalink":"https://rcfduarte.github.io/publication/weidel_2020/","publishdate":"2021-03-04T00:00:00Z","relpermalink":"/publication/weidel_2020/","section":"publication","summary":"Reinforcement learning is a learning paradigm that can account for how organisms learn to adapt their behavior in complex environments with sparse rewards. However, implementations in spiking neuronal networks typically rely on input architectures involving place cells or receptive fields. This is problematic, as such approaches either scale badly as the environment grows in size or complexity, or presuppose knowledge on how the environment should be partitioned. Here, we propose a learning architecture that combines unsupervised learning on the input projections with clustered connectivity within the representation layer. This combination allows input features to be mapped to clusters; thus the network self-organizes to produce task-relevant activity patterns that can serve as the basis for reinforcement learning on the output projections. On the basis of the MNIST and Mountain Car tasks, we show that our proposed model performs better than either a comparable unclustered network or a clustered network with static input projections. We conclude that the combination of unsupervised learning and clustered connectivity provides a generic representational substrate suitable for further computation.","tags":["Connectivity","Learning","Clusters","Processing"],"title":"Unsupervised learning and clustered connectivity enhance reinforcement learning in spiking neural networks","type":"publication"},{"authors":["Barna Zajzon","Sepehr Mahmoudian","Abigail Morrison","Renato Duarte"],"categories":null,"content":"","date":1575504000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575504000,"objectID":"42c41f76576f47b722866f5810b78ffb","permalink":"https://rcfduarte.github.io/publication/state_transfer_2019/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/state_transfer_2019/","section":"publication","summary":"Neurobiological systems rely on hierarchical and modular architectures to carry out intricate computations using minimal resources. A prerequisite for such systems to operate adequately is the capability to reliably and efficiently transfer information across multiple modules. Here, we study the features enabling a robust transfer of stimulus representations in modular networks of spiking neurons, tuned to operate in a balanced regime. To capitalize on the complex, transient dynamics that such networks exhibit during active processing, we apply reservoir computing principles and probe the systems' computational efficacy with specific tasks. Focusing on the comparison of random feed-forward connectivity and biologically inspired topographic maps, we find that, in a sequential set-up, structured projections between the modules are strictly necessary for information to propagate accurately to deeper modules. Such mappings not only improve computational performance and efficiency, they also reduce response variability, increase robustness against interference effects, and boost memory capacity. We further investigate how information from two separate input streams is integrated and demonstrate that it is more advantageous to perform non-linear computations on the input locally, within a given module, and subsequently transfer the result downstream, rather than transferring intermediate information and performing the computation downstream. Depending on how information is integrated early on in the system, the networks achieve similar task-performance using different strategies, indicating that the dimensionality of the neural responses does not necessarily correlate with nonlinear integration, as predicted by previous studies. These findings highlight a key role of topographic maps in supporting fast, robust, and accurate neural communication over longer distances. Given the prevalence of such structural feature, particularly in the sensory systems, elucidating their functional purpose remains an important challenge toward which this work provides relevant, new insights. At the same time, these results shed new light on important requirements for designing functional hierarchical spiking networks.","tags":["Modularity","Information transfer","Stimulus representation"],"title":"Passing the message: Representation transfer in modular balanced networks","type":"publication"},{"authors":[""],"categories":null,"content":"","date":1558447200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558447200,"objectID":"6e7cdabd34d4256ed8dc4bda9c01ca85","permalink":"https://rcfduarte.github.io/talk/shonan/","publishdate":"2019-05-05T00:00:00Z","relpermalink":"/talk/shonan/","section":"talk","summary":"Several organizational principles of the neocortex appear to imply a strong predisposition to acquire temporal structure in a completely incidental/unsupervised manner, a process that is central to many core aspects of cognition. In this work, we explore the processes involved in implicit, structured sequence learning in biologically-inspired architectures, systems where the current state continuously interacts with and modifies the processing characteristics. We demonstrate a prominent role of synaptic plasticity (particularly of inhibitory synapses) in representational and rule-guided learning, an effect achieved by maintaining compact dynamic representations and sparse, distributed activity patterns. We highlight a form of sequential metastability as a potential mechanism for sequence processing in neocortical circuits. In addition, I will discuss how innate constraints in the patterning of the synaptic machinery throughout the neocortex may bias a circuit’s intrinsic timescales and memory capacity, while the high degree of complexity and heterogeneity may serve important computational purposes by expanding the circuit’s functional space.","tags":[],"title":"State-dependent processing in Spiking Neural Networks","type":"talk"},{"authors":["Renato Duarte","Abigail Morrison"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"b22fe5dd6985560ceee8c38e172097c1","permalink":"https://rcfduarte.github.io/publication/heterogeneity_2019/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/heterogeneity_2019/","section":"publication","summary":"Complexity and heterogeneity are intrinsic to neurobiological systems, manifest in every process, at every scale, and are inextricably linked to the systems’ emergent collective behaviours and function. However, the majority of studies addressing the dynamics and computational properties of biologically inspired cortical microcircuits tend to assume (often for the sake of analytical tractability) a great degree of homogeneity in both neuronal and synaptic/connectivity parameters. While simplification and reductionism are necessary to understand the brain’s functional principles, disregarding the existence of the multiple heterogeneities in the cortical composition, which may be at the core of its computational proficiency, will inevitably fail to account for important phenomena and limit the scope and generalizability of cortical models. We address these issues by studying the individual and composite functional roles of heterogeneities in neuronal, synaptic and structural properties in a biophysically plausible layer 2/3 microcircuit model, built and constrained by multiple sources of empirical data. This approach was made possible by the emergence of large-scale, well curated databases, as well as the substantial improvements in experimental methodologies achieved over the last few years. Our results show that variability in single neuron parameters is the dominant source of functional specialization, leading to highly proficient microcircuits with much higher computational power than their homogeneous counterparts. We further show that fully heterogeneous circuits, which are closest to the biophysical reality, owe their response properties to the differential contribution of different sources of heterogeneity.","tags":["Heterogeneity","Neocortex","Information processing","Fading memory"],"title":"Leveraging heterogeneity for neural computation with fading memory in layer 2/3 cortical microcircuits","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://rcfduarte.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Barna Zajzon","Renato Duarte","Abigail Morrison"],"categories":null,"content":"","date":1539561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539561600,"objectID":"e992e0c87a07890324dcd86be2d38c95","permalink":"https://rcfduarte.github.io/publication/statetransfer_ijcnn/","publishdate":"2018-10-15T00:00:00Z","relpermalink":"/publication/statetransfer_ijcnn/","section":"publication","summary":"Hierarchical modularity is a parsimonious design principle in many complex systems and underlies various key structural and functional aspects of neurobiological systems, whose modules are recurrent networks of spiking neurons. An essential requirement for such systems to adequately function is the ability to transfer information across multiple modules in a reliable and efficient manner. In this work, we study the characteristics of emergent stimulus representations in recurrent, spiking neural networks and the features that allow efficient information transfer among multiple, interacting sub-networks. We find that the specificity of structural mappings between the modules is strictly required for information to propagate to a sufficient depth, in a sequential setup. Conserved topography not only improves computational performance in all scenarios analyzed, but it proves to be more robust against noise and interference effects, results in less variability in the neural responses and increases memory capacity.","tags":["Stimulus representation","State transfer","Modularity","Spiking Neural Networks"],"title":"Transferring state representations in hierarchical spiking neural networks","type":"publication"},{"authors":[""],"categories":null,"content":"","date":1531180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531180800,"objectID":"3c20d1c3ce4865b9d17f5e28c89a6f14","permalink":"https://rcfduarte.github.io/talk/ed_poster/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/ed_poster/","section":"talk","summary":"In this work, we investigate the representational capacity of spiking networks engaged in an identity mapping task. We compare two schemes for encoding symbolic input, one in which input is injected as a direct current and one where input is delivered as a spatio-temporal spike pattern. We also compare performance using either membrane potentials or filtered spike trains as state variable. Furthermore, we investigate how the circuit behavior depends on the balance between excitation and inhibition, and the degree of synchrony and regularity in its internal dynamics. Finally, we compare different linear methods of decoding population activity onto desired target labels. Overall, our results suggest that even this simple mapping task is strongly influenced by design choices on input encoding, state-variables, circuit characteristics and decoding methods, and these factors can interact in complex ways.","tags":[],"title":"Encoding symbolic sequences with spiking neural reservoirs","type":"talk"},{"authors":["Renato Duarte","Marvin Uhlmann","Dick van den Broek","Hartmut Fitz","Karl Magnus Petersson","Abigail Morrison"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"61cc1cffe3a6823a85ba53dfafc9ec1c","permalink":"https://rcfduarte.github.io/publication/ed_2018/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/ed_2018/","section":"publication","summary":"Biologically inspired spiking networks are an important tool to study the nature of computation and cognition in neural systems. In this work, we investigate the representational capacity of spiking networks engaged in an identity mapping task. We compare two schemes for encoding symbolic input, one in which input is injected as a direct current and one where input is delivered as a spatio-temporal spike pattern. We test the ability of networks to discriminate their input as a function of the number of distinct input symbols. We also compare performance using either membrane potentials or filtered spike trains as state variable. Furthermore, we investigate how the circuit behavior depends on the balance between excitation and inhibition, and the degree of synchrony and regularity in its internal dynamics. Finally, we compare different linear methods of decoding population activity onto desired target labels. Overall, our results suggest that even this simple mapping task is strongly influenced by design choices on input encoding, state-variables, circuit characteristics and decoding methods, and these factors can interact in complex ways. This work highlights the importance of constraining computational network models of behavior by available neurobiological evidence.","tags":["Stimulus encoding","Reservoir computing","Balanced random networks"],"title":"Encoding symbolic sequences with spiking neural reservoirs","type":"publication"},{"authors":[""],"categories":null,"content":"","date":1526473800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526473800,"objectID":"cce31cb08be843182a9d15e55854ae20","permalink":"https://rcfduarte.github.io/talk/cbmr/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/cbmr/","section":"talk","summary":"Many different aspects of cognitive function express themselves as structured temporal sequences. On the other hand, several important organizational principles of the neocortex appear to imply a strong predisposition to acquire this temporal structure in a completely incidental/unsupervised manner. In this work, we have explored the processes involved in implicit, structured sequence learning in biologically-inspired architectures in order to evaluate the character of on-line processing memory and finite precision computation in systems where the current state continuously interacts with and modifies the processing characteristics. We have demonstrated a prominent role of synaptic plasticity (particularly of inhibitory synapses) in representational and rule-guided learning, an effect achieved by maintaining compact dynamic representations and sparse, distributed activity patterns. We have highlighted a form of sequential metastability as a potential mechanism for sequence learning in neocortical circuits. In addition, we discuss how innate constraints in the patterning of the synaptic machinery throughout the neocortex may bias a circuit’s intrinsic timescales and memory capacity, while the high degree of complexity and heterogeneity may serve important computational purposes by expanding the circuit’s functional space.","tags":[],"title":"State-dependent processing in Spiking Neural Networks","type":"talk"},{"authors":[""],"categories":null,"content":"","date":1525356000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525356000,"objectID":"0d2231d4aab4517c20be07b5717c51ac","permalink":"https://rcfduarte.github.io/talk/phd_defence/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/phd_defence/","section":"talk","summary":"Many different aspects of cognitive function express themselves as structured temporal sequences. On the other hand, several important organizational principles of the neocortex appear to imply a strong predisposition to acquire this temporal structure in a completely incidental/unsupervised manner. Throughout this thesis, we have explored the processes involved in implicit, structured sequence learning in biologically-inspired architectures in order to evaluate the character of on-line processing memory and finite precision computation in systems where the current state continuously interacts with and modifies the processing characteristics. We have demonstrated a prominent role of synaptic plasticity (particularly of inhibitory synapses) in representational and rule-guided learning, an effect achieved by maintaining compact dynamic representations and sparse, distributed activity patterns. We have highlighted a form of sequential metastability as a potential mechanism for sequence learning in neocortical circuits. In addition, we discuss how innate constraints in the patterning of the synaptic machinery throughout the neocortex may bias a circuit's intrinsic timescales and memory capacity, while the high degree of complexity and heterogeneity may serve important computational purposes by expanding the circuit's functional space.","tags":[],"title":"State-dependent processing in Spiking Neural Networks","type":"talk"},{"authors":["Renato Duarte"],"categories":null,"content":"","date":1513468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513468800,"objectID":"f82f69cc39afae3c04d7e09ec8ff8cf4","permalink":"https://rcfduarte.github.io/publication/phd_thesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/phd_thesis/","section":"publication","summary":"Cognitive and behavioral processes are indissociable from their biophysical substrateand the characteristics of the underlying processing elements. As such, neural computation and the properties of functional neurodynamics ought to be understood primarily as complex biophysical phenomena: nested interactions, spanning multiple temporaland spatial scales and distributed across massively parallel modular hierarchies. The observable dynamics, both at a mesoscopic and microscopic scale, are the result of complex nonlinear interactions, the ensemble actions of very large and heterogeneous neuronal populations. These, shaped by evolutionary and developmental constraints and permanently subjected to functional re-organization, constitute very proficient adaptive processing systems. To a first approximation, this complex circuitry can be seen as large excitable reservoirs, whose symmetry-breaking inhomogeneities (present at multiple levels) naturally give rise to rich high-dimensional dynamics that supports cognitive function and computation. Furthermore, the intrinsic recurrent dynamics endows the system with fading memory, but places critical constraints on processing precision. Neural computation ought to be studied in context, accounting for the emergence of mental phenomena and thus guided and constrained by findings from the cognitive and behavioral sciences. These can provide tasks and computational specifications, as well as performance constraints while the necessary parallels between structure and function are gradually and systematically established. In this context, it is reasonable to study the implementation of different aspects of cognitive function that express themselves as temporal sequences given that they are ubiquitous in multiple cognitive domains (from sensorimotor sequencing to language processing). Interestingly, the cortical architecture and its underlying functional relations also appear to be highly sensitive to serial order and temporal structure. The ability to perceive statistically repeating spatiotemporal patterns and abstract the underlying rules may constitute a domain-general mechanism for the acquisition of predictive relations, as it allows for efficient generalization over compact representations. The ever-changing network of synaptic (and neuronal intrinsic) properties determines, on short to medium timescales, the ability of a circuit to process time-varying input in an active, predictive manner, dynamically patterned as transient sequences of network states which, through the orchestration of multiple plasticity mechanisms, come to reflect an increasingly restricted and accurate internal model of the relevant knowledge structure. Throughout this thesis, we propose to model the underlying systems (at various degrees of biological plausibility) in functional contexts, in order to gain knowledge about the system itself and the computational relevance of its internal features. We attempt to shed light on the nature of on-line integration of information, while evaluating the character of on-line processing memory and finite precision computation in systems where the current state continuously interacts with and modifies the processing characteristics.","tags":["Recurrent Neural Networks","Reservoir Computing","Plasticity","Self-organization"],"title":"State-dependent processing in Spiking Neural Networks","type":"publication"},{"authors":["Dick van den Broek","Marvin Uhlmann","Renato Duarte","Hartmut Fitz","Peter Hagoort","Karl Magnus Petersson"],"categories":null,"content":"","date":1508025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508025600,"objectID":"ce0cb4e7ec2d0afd92e67fe8ca497a40","permalink":"https://rcfduarte.github.io/publication/spike_filter_kernel_ccn2017/","publishdate":"2018-10-15T00:00:00Z","relpermalink":"/publication/spike_filter_kernel_ccn2017/","section":"publication","summary":"A common approach to extracting information from simulated spiking neural networks is to train readouts on a spike-rate variable obtained through convolution of output spike-trains with a filter. Here we argue that best practice is to use neurons as spike filters. We describe how neural circuits consist of stock and flow variables that co-determine each other and argue that membrane potentials provide access to the information contained in the circuit in a more natural and unbiased way than filtered spike-trains. We compare the two different approaches to readout calibration in a classification task.","tags":["Spiking Neural Networks","Decoding","Spike-train filter","Membrane potential","Stock-flow duality"],"title":"The best spike filter kernel is a neuron","type":"publication"},{"authors":[""],"categories":null,"content":"","date":1505606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505606400,"objectID":"29413a260ef55dc8303644c31173092e","permalink":"https://rcfduarte.github.io/talk/het_poster_nccd/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/het_poster_nccd/","section":"talk","summary":"Computational studies addressing the dynamics and computational properties of biologically inspired spiking neurons and networks tend to assume (often for the sake of analytical tractability) a great degree of homogeneity in both neuronal and connectivity parameters. The biophysical reality, however, is radically different from a homogeneous system and multiple levels of complex heterogeneous properties co-exist and shape a local circuit’s emergent collective dynamics and information processing properties. Within each cortical module, the characteristic patterning of the microcircuit’s building blocks and their mechanistic interactions give rise to rich dynamics, which subserves local computation by shaping the spatiotemporal features of population responses. Despite their varying molecular, morphological and physiological features, cortical modules can be seen as variations on a common theme. In essence, notwithstanding the complex laminar patterning and differential input-output relations which might give rise to additional structural and functional sub-parcellations, cortical modules are large recurrently coupled neuronal networks, whose interactions are achieved primarily via spike-triggered excitatory and inhibitory transmission. The combined complexity of these heterogeneous building blocks can be leveraged by cortical microcircuits to provide a rich dynamical space where complex relational constructs, spanning multiple timescales, can be learned, represented and used for online information processing. In this study, we set out to systematically evaluate the role played by different sources of heterogeneity (structural, neuronal and synaptic) in the characteristics of population dynamics and the circuit’s capacity for online stimulus processing with fading memory, using cortical layer 2/3 microcircuits as a core inspiration for the circuit specification. We cross-reference various sources of experimental data regarding the composition and patterning of these microcircuits, accounting for different phenomena of interest (e.g. neuron types and corresponding sub-threshold characteristics, conductance properties of different receptor types, circuit-level connectivity and activity statistics, etc.), across different cortical regions, assuming a certain degree of generalization is possible. The methods applied in this study to quantify the dynamics and generic processing properties, being system-independent, can provide a valuable set of tools for microcircuit benchmarking. As carefully curated and organized datasets become increasingly available, it will become possible in the near future to apply increasingly realistic constrains and comparatively study the properties of realistic microcircuits, built to model specific cortical regions and input-output relations.","tags":[],"title":"Leveraging heterogeneity for neural computation with fading memory","type":"talk"},{"authors":[""],"categories":null,"content":"","date":1504742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504742400,"objectID":"dd7ebc2e5b7aff8ab8a76c1099cf863b","permalink":"https://rcfduarte.github.io/talk/het_poster_isn/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/het_poster_isn/","section":"talk","summary":"Computational studies addressing the dynamics and computational properties of biologically inspired spiking neurons and networks tend to assume (often for the sake of analytical tractability) a great degree of homogeneity in both neuronal and connectivity parameters. The biophysical reality, however, is radically different from a homogeneous system and multiple levels of complex heterogeneous properties co-exist and shape a local circuit’s emergent collective dynamics and information processing properties. Within each cortical module, the characteristic patterning of the microcircuit’s building blocks and their mechanistic interactions give rise to rich dynamics, which subserves local computation by shaping the spatiotemporal features of population responses. Despite their varying molecular, morphological and physiological features, cortical modules can be seen as variations on a common theme. In essence, notwithstanding the complex laminar patterning and differential input-output relations which might give rise to additional structural and functional sub-parcellations, cortical modules are large recurrently coupled neuronal networks, whose interactions are achieved primarily via spike-triggered excitatory and inhibitory transmission. The combined complexity of these heterogeneous building blocks can be leveraged by cortical microcircuits to provide a rich dynamical space where complex relational constructs, spanning multiple timescales, can be learned, represented and used for online information processing. In this study, we set out to systematically evaluate the role played by different sources of heterogeneity (structural, neuronal and synaptic) in the characteristics of population dynamics and the circuit’s capacity for online stimulus processing with fading memory, using cortical layer 2/3 microcircuits as a core inspiration for the circuit specification. We cross-reference various sources of experimental data regarding the composition and patterning of these microcircuits, accounting for different phenomena of interest (e.g. neuron types and corresponding sub-threshold characteristics, conductance properties of different receptor types, circuit-level connectivity and activity statistics, etc.), across different cortical regions, assuming a certain degree of generalization is possible. The methods applied in this study to quantify the dynamics and generic processing properties, being system-independent, can provide a valuable set of tools for microcircuit benchmarking. As carefully curated and organized datasets become increasingly available, it will become possible in the near future to apply increasingly realistic constrains and comparatively study the properties of realistic microcircuits, built to model specific cortical regions and input-output relations.","tags":[],"title":"Leveraging heterogeneity for neural computation with fading memory","type":"talk"},{"authors":[""],"categories":null,"content":"","date":1499817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499817600,"objectID":"e8d712df978ef2770adb8a7579b49797","permalink":"https://rcfduarte.github.io/talk/het_poster_inm_ics2017/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/het_poster_inm_ics2017/","section":"talk","summary":"Computational studies addressing the dynamics and computational properties of biologically inspired spiking neurons and networks tend to assume (often for the sake of analytical tractability) a great degree of homogeneity in both neuronal and connectivity parameters. The biophysical reality, however, is radically different from a homogeneous system and multiple levels of complex heterogeneous properties co-exist and shape a local circuit’s emergent collective dynamics and information processing properties. Within each cortical module, the characteristic patterning of the microcircuit’s building blocks and their mechanistic interactions give rise to rich dynamics, which subserves local computation by shaping the spatiotemporal features of population responses. Despite their varying molecular, morphological and physiological features, cortical modules can be seen as variations on a common theme. In essence, notwithstanding the complex laminar patterning and differential input-output relations which might give rise to additional structural and functional sub-parcellations, cortical modules are large recurrently coupled neuronal networks, whose interactions are achieved primarily via spike-triggered excitatory and inhibitory transmission. The combined complexity of these heterogeneous building blocks can be leveraged by cortical microcircuits to provide a rich dynamical space where complex relational constructs, spanning multiple timescales, can be learned, represented and used for online information processing. In this study, we set out to systematically evaluate the role played by different sources of heterogeneity (structural, neuronal and synaptic) in the characteristics of population dynamics and the circuit’s capacity for online stimulus processing with fading memory, using cortical layer 2/3 microcircuits as a core inspiration for the circuit specification. We cross-reference various sources of experimental data regarding the composition and patterning of these microcircuits, accounting for different phenomena of interest (e.g. neuron types and corresponding sub-threshold characteristics, conductance properties of different receptor types, circuit-level connectivity and activity statistics, etc.), across different cortical regions, assuming a certain degree of generalization is possible. The methods applied in this study to quantify the dynamics and generic processing properties, being system-independent, can provide a valuable set of tools for microcircuit benchmarking. As carefully curated and organized datasets become increasingly available, it will become possible in the near future to apply increasingly realistic constrains and comparatively study the properties of realistic microcircuits, built to model specific cortical regions and input-output relations.","tags":[],"title":"Leveraging complexity and heterogeneity for neural computation with fading memory","type":"talk"},{"authors":["Renato Duarte","Alexander Seeholzer","Karl Zilles","Abigail Morrison"],"categories":null,"content":"","date":1491782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491782400,"objectID":"15cf70f69c4ac5c61325489d95ea4232","permalink":"https://rcfduarte.github.io/publication/coneur_2017/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/coneur_2017/","section":"publication","summary":"Neocortical circuits, as large heterogeneous recurrentnetworks, can potentially operate and process signals atmultiple timescales, but appear to be differentially tuned tooperate within certain temporal receptive windows. Themodular and hierarchical organization of this selectivity mirrorsanatomical and physiological relations throughout the cortexand is likely determined by the regional electrochemicalcomposition. Being consistently patterned and activelyregulated, the expression of molecules involved in synaptictransmission constitutes the most significant source of laminarand regional variability. Due to their complex kinetics andadaptability, synapses form a natural primary candidateunderlying this regional temporal selectivity. The ability ofcortical networks to reflect the temporal structure of thesensory environment can thus be regulated by evolutionary andexperience-dependent processes.","tags":null,"title":"Synaptic patterning and the timescales of cortical dynamics","type":"publication"},{"authors":[""],"categories":null,"content":"","date":1481018400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481018400,"objectID":"3f70e743f09f2a46370aad3d99860fc5","permalink":"https://rcfduarte.github.io/talk/ias_symposium_2016/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/ias_symposium_2016/","section":"talk","summary":"Computational studies addressing the dynamics and computational properties of biologically inspired spiking neurons and networks tend to assume (often for the sake of analytical tractability) a great degree of homogeneity in both neuronal and connectivity parameters. The biophysical reality, however, is radically different from a homogeneous system and multiple levels of complex heterogeneous properties co-exist and shape a local circuit’s emergent collective dynamics and information processing properties. Within each cortical module, the characteristic patterning of the microcircuit’s building blocks and their mechanistic interactions give rise to rich dynamics, which subserves local computation by shaping the spatiotemporal features of population responses. Despite their varying molecular, morphological and physiological features, cortical modules can be seen as variations on a common theme. In essence, notwithstanding the complex laminar patterning and differential input-output relations which might give rise to additional structural and functional sub-parcellations, cortical modules are large recurrently coupled neuronal networks, whose interactions are achieved primarily via spike-triggered excitatory and inhibitory transmission. The combined complexity of these heterogeneous building blocks can be leveraged by cortical microcircuits to provide a rich dynamical space where complex relational constructs, spanning multiple timescales, can be learned, represented and used for online information processing. In this study, we set out to systematically evaluate the role played by different sources of heterogeneity (structural, neuronal and synaptic) in the characteristics of population dynamics and the circuit’s capacity for online stimulus processing with fading memory, using cortical layer 2/3 microcircuits as a core inspiration for the circuit specification. We cross-reference various sources of experimental data regarding the composition and patterning of these microcircuits, accounting for different phenomena of interest (e.g. neuron types and corresponding sub-threshold characteristics, conductance properties of different receptor types, circuit-level connectivity and activity statistics, etc.), across different cortical regions, assuming a certain degree of generalization is possible. The methods applied in this study to quantify the dynamics and generic processing properties, being system-independent, can provide a valuable set of tools for microcircuit benchmarking. As carefully curated and organized datasets become increasingly available, it will become possible in the near future to apply increasingly realistic constrains and comparatively study the properties of realistic microcircuits, built to model specific cortical regions and input-output relations.","tags":[],"title":"Leveraging heterogeneity for neural computation with fading memory","type":"talk"},{"authors":[""],"categories":null,"content":"","date":1476266400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476266400,"objectID":"9f921542da00161e341505a583d5aaae","permalink":"https://rcfduarte.github.io/talk/inm_retreat_2016/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/inm_retreat_2016/","section":"talk","summary":"Perceptual decision-making is an intricate process implicating the coordinated activity of multiple brain areas. Recent experimental studies demonstrate the existence of a complex interplay between decision-related neural events and transient working memory processes, implemented by distributed circuits where specific sub-populations appear to be differentially involved in the evidence accumulation process and subsequent behavioral outcomes. This results in observable divergences in choice-specific neuronal dynamics, unfolding as reproducible trajectories throughout the network’s state-space and hinting at the dissipative nature of the underlying dynamical system, which executes cognitively relevant processing through transient trajectories. Despite this evidence, the majority of modeling studies addressing reward-modulated decision-making tend to simplify the formalization of environmental representations in the cortex as stable, attractor states corresponding to discrete environmental states. Even models involving transient-based computations often simplify sensory stimuli to a discrete set of inputs transduced as stochastic point processes. These simplifications potentially draw an incomplete picture of neural dynamics and therefore provide limited insights into the true nature of computation in neural circuits. To overcome this issue, we take one step towards realistic in silico experimental settings by using structured virtual environments to obtain rich sensory input to drive model neural systems using the ROS-MUSIC toolchain. It allows us to simulate robotic agents in virtual 3D environments performing a realistic perceptual decision task, which can be directly equated to experimental data. The robotic simulation generates realistic and structured sensory data which is encoded to spiking neural activity using a nonlinear encoding process. The encoded sensory data is then used as input to a balance recurrent neural circuit. In this study, we investigate the emergent dynamical features of neural activity when the agent is navigating a virtual T-maze. We observe decision-specific sequences of neural activity akin to experimental evidence, revealing possible processing strategies employed by the neural substrate. Furthermore, we investigate the role of different adaptation/plasticity mechanisms in shaping the system’s dynamics. In order to equate our results with those of other studies, we attempt to partition the network state-space into discrete activity clusters, which carry relevant information that could potentially be used to drive reinforcement learning algorithms.","tags":[],"title":"Decision-specific sequences of neural activity in balanced random networks driven by structured sensory input","type":"talk"},{"authors":["Philipp Weidel","Mikael Djurfeldt","Renato Duarte","Abigail Morrison"],"categories":null,"content":"","date":1470182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470182400,"objectID":"7e71b3b71b3f6b663eb86d7a505057c4","permalink":"https://rcfduarte.github.io/publication/ros_music/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/ros_music/","section":"publication","summary":"In order to properly assess the function and computational properties of simulated neural systems, it is necessary to account for the nature of the stimuli that drive the system. However, providing stimuli that are rich and yet both reproducible and amenable to experimental manipulations is technically challenging, and even more so if a closed-loop scenario is required. In this work, we present a novel approach to solve this problem, connecting robotics and neural network simulators. We implement a middleware solution that bridges the Robotic Operating System (ROS) to the Multi-Simulator Coordinator (MUSIC). This enables any robotic and neural simulators that implement the corresponding interfaces to be efficiently coupled, allowing real-time performance for a wide range of configurations. This work extends the toolset available for researchers in both neurorobotics and computational neuroscience, and creates the opportunity to perform closed-loop experiments of arbitrary complexity to address questions in multiple areas, including embodiment, agency, and reinforcement learning.","tags":["Heterogeneity","Neocortex","Processing"],"title":"Closed loop interactions between spiking neural network and robotic simulators based on MUSIC and ROS","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://rcfduarte.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":[""],"categories":null,"content":"","date":1437436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437436800,"objectID":"08bf2edd7405cb8467958cdee41750a8","permalink":"https://rcfduarte.github.io/talk/ros_music_cns2015/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/ros_music_cns2015/","section":"talk","summary":"Studying a functional, biologically plausible neural network that performs a particular task is highly relevant for progress in both neuroscience and machine learning. Most tasks used to test the function of a simulated neural network are still very artificial and thus too narrow, providing only little insight into the true value of a particular neural network architecture under study. For example, many models of reinforcement learning in the brain rely on a discrete set of environmental states and actions. In order to move closer towards more realistic models, modeling studies have to be conducted in more realistic environments that provide complex sensory input about the states. A way to achieve this is to provide an interface between a robotic and a neural network simulation, such that a neural network controller gains access to a realistic agent which is acting in a complex environment that can be flexibly designed by the experimentalist.\nTo create such an interface, we present a toolchain, consisting of already existing and robust tools, which forms the missing link between robotic and neuroscience with the goal of connecting robotic simulators with neural simulators. This toolchain is a generic solution and is able to combine various robotic simulators with various neural simulators by connecting the Robot Operating System (ROS) with the Multi-Simulation Coordinator (MUSIC). ROS is the most widely used middleware in the robotic community with interfaces for robotic simulators like Gazebo, Morse, Webots, etc, and additionally allows the users to specify their own robot and sensors in great detail with the Unified Robot Description Language (URDF). MUSIC is a communicator between the major, state-of-the-art neural simulators: NEST, Moose and NEURON. By implementing an interface between ROS and MUSIC, our toolchain is combining two powerful middlewares, and is therefore a multi-purpose generic solution.\nOne main purpose is the translation from continuous sensory data, obtained from the sensors of a virtual robot, to spiking data which is passed to a neural simulator of choice. The translation from continuous data to spiking data is performed using the Neural Engineering Framework (NEF) proposed by Eliasmith \u0026 Anderson. By sending motor commands from the neural simulator back to the robotic simulator, the interface is forming a closed loop between the virtual robot and its spiking neural network controller.\nTo demonstrate the functionality of the toolchain and the interplay between all its different components, we implemented one of the vehicles described by Braitenberg using the robotic simulator Gazebo and the neural simulator NEST.\nIn future work, we aim to create a testbench, consisting of various environments for reinforcement learning algorithms, to provide a validation tool for the functionality of biological motivated models of learning.","tags":[],"title":"ROS-MUSIC Toolchain for Spiking Neural Network Simulations in a Robotic Environment","type":"talk"},{"authors":[""],"categories":null,"content":"","date":1432382400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1432382400,"objectID":"4a03be0df174c3adc260f066c8884849","permalink":"https://rcfduarte.github.io/talk/eurospin_2015/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/eurospin_2015/","section":"talk","summary":"In this work, we study the properties of biologically realistic networks of LIF neurons, with differentially modulated, dynamic excitation and inhibition, combining well established as well as more recent phenomenological models of synaptic plasticity. We begin by demonstrating that timing-dependent synaptic plasticity mechanisms have an important role to play in the active maintenance of an ongoing dynamics characterized by asynchronous and irregular firing, closely resembling cortical activity in vivo. Incoming stimuli, acting as perturbations of the local balance of excitation and inhibition, require fast adaptive responses to prevent the development of unstable activity regimes, which we objectively link between to a reduced generic computational capacity. Additionally, we demonstrate that the action of plasticity shapes and stabilizes the transient network states exhibited in the presence of sequentially presented stimulus events, allowing the development of adequate and discernible stimulus representations. The main feature responsible for the increased discriminability of stimulus-driven population responses in plastic networks is shown to be the decorrelating action of inhibitory plasticity and the consequent maintenance of the asynchronous irregular dynamic regime both for ongoing activity and stimulus-driven responses, whereas excitatory plasticity is shown to play only a marginal role.","tags":[],"title":"Synaptic adaptation stabilizes sequential stimulus representations","type":"talk"},{"authors":["Renato Duarte"],"categories":null,"content":"","date":1431475200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1431475200,"objectID":"8eae2b034ad2f78dcd66ccf3d8a58319","permalink":"https://rcfduarte.github.io/publication/jneuro_2015/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/jneuro_2015/","section":"publication","summary":"The fidelity with which cortical sensory areas represent environmental stimuli poses a strong constraint on the quality, suitability, and precision of any subsequent computation and, ultimately, on the organism's behavioral performance. To shed some light into the origins of trial-to-trial variability in population responses and its relation to ongoing cortical states, this journal club article reviews the work of Scholvinck et al. (2015).","tags":["Sensory cortex","Response variability","State-dependence","Expansion"],"title":"Expansion and State-Dependent Variability along Sensory Processing Streams","type":"publication"},{"authors":[""],"categories":null,"content":"","date":1418829000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1418829000,"objectID":"583e04f8638a13155dffff671d3f983f","permalink":"https://rcfduarte.github.io/talk/gso2014/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/gso2014/","section":"talk","summary":"In this work, we study the properties of biologically realistic networks of LIF neurons, with differentially modulated, dynamic excitation and inhibition, combining well established as well as more recent phenomenological models of synaptic plasticity. We begin by demonstrating that timing-dependent synaptic plasticity mechanisms have an important role to play in the active maintenance of an ongoing dynamics characterized by asynchronous and irregular firing, closely resembling cortical activity in vivo. Incoming stimuli, acting as perturbations of the local balance of excitation and inhibition, require fast adaptive responses to prevent the development of unstable activity regimes, which we objectively link between to a reduced generic computational capacity. Additionally, we demonstrate that the action of plasticity shapes and stabilizes the transient network states exhibited in the presence of sequentially presented stimulus events, allowing the development of adequate and discernible stimulus representations. The main feature responsible for the increased discriminability of stimulus-driven population responses in plastic networks is shown to be the decorrelating action of inhibitory plasticity and the consequent maintenance of the asynchronous irregular dynamic regime both for ongoing activity and stimulus-driven responses, whereas excitatory plasticity is shown to play only a marginal role.","tags":[],"title":"Synaptic adaptation stabilizes sequential stimulus representations","type":"talk"},{"authors":[""],"categories":null,"content":"","date":1417046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1417046400,"objectID":"d93001e56d7bfee014657df48e0fdd0d","permalink":"https://rcfduarte.github.io/talk/hbp_poster_2014/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/hbp_poster_2014/","section":"talk","summary":"In this work, we numerically explore the quality and spatiotemporal characteristics of dynamic stimulus representations in inhibition dominated, sparsely coupled recurrent networks of IF neurons, as well as the role played by ongoing, background activity in active processing and computation. We assume that the characteristics of stimulus representations are highly dependent on the current state of the circuit upon stimulus arrival and thus necessarily bound to the properties of ongoing, background activity. Our networks are additionally endowed with a combination of timing-dependent synaptic plasticity mechanisms which provide an ongoing modulation of the balance of E/I. We begin by assessing the impact of plasticity on ongoing activity, demonstrating that it greatly increases the robustness of asynchronous irregular states, which are characterized by Poison-like population activity. In the presence of plasticity, this pattern is observed in a much broader  parameter range when compared to networks whose synapses are fixed and static. To perform this analysis, we systematically vary two control parameters: the relative strengths of E/I and the rate of a constant, unspecific and stochastic input). Plasticity is also shown to completely abolish states of synchronous regular population activity. We additionally demonstrate that these pathological states greatly reduced the network's capacity for online processing of time-varying input streams (or the network's kernel quality). On the opposite end, the stochastic AI states are related to a greater computational capacity. By increasing the robustness of this activity profile, the combined action of these plasticity mechanisms is shown to improve generic computational capacity. Subsequently, the networks are stimulated with a series of topographically mapped input pulses, formalized as  a series of independent inhomogeneous Poisson processes whose rates are determined by these impulse kernels and whose activity is delivered to specifically tunned sub-populations within the main network. Under biologically plausible input conditions, the action of plasticity is demonstrated to improve robustness and reproducibility of the emergent stimulus-specific spatiotemporal response transients, allowing a population of linear readouts to discriminate the identity of the stimuli throughout the entire time course of the observed responses. This effect is shown to be mainly justified by the decorrelating effects of iSTDP, which actively maintains the AI activity profile both for ongoing activity and stimulus-driven responses. This stochastic firing pattern allows the network to more efficiently explore its state-space, and lies in opposition to the increasingly redundant and restrictive dynamical state observed in static networks, where states of greater synchrony and regularity come to dominate the responses.","tags":[],"title":"Dynamic stimulus representations in adapting neuronal networks","type":"talk"},{"authors":["Carlos Toledo-Suárez","Renato Duarte","Abigail Morrison"],"categories":null,"content":"","date":1416528000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1416528000,"objectID":"494166f0240579a98f41460b0f3c66ab","permalink":"https://rcfduarte.github.io/publication/striatum_lsm/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/striatum_lsm/","section":"publication","summary":"In reinforcement learning theories of the basal ganglia, there is a need for the expected rewards corresponding to relevant environmental states to be maintained and modified during the learning process. However, the representation of these states that allows them to be associated with reward expectations remains unclear. Previous studies have tended to rely on pre-defined partitioning of states encoded by disjunct neuronal groups or sparse topological drives. A more likely scenario is that striatal neurons are involved in the encoding of multiple different states through their spike patterns, and that an appropriate partitioning of an environment is learned on the basis of task constraints, thus minimizing the number of states involved in solving a particular task. Here we show that striatal activity is sufficient to implement a liquid state, an important prerequisite for such a computation, whereby transient patterns of striatal activity are mapped onto the relevant states. We develop a simple small scale model of the striatum which can reproduce key features of the experimentally observed activity of the major cell types of the striatum. We then use the activity of this network as input for the supervised training of four simple linear readouts to learn three different functions on a plane, where the network is stimulated with the spike coded position of the agent. We discover that the network configuration that best reproduces striatal activity statistics lies on the edge of chaos and has good performance on all three tasks, but that in general, the edge of chaos is a poor predictor of network performance.","tags":["Striatum","Liquid State Machine","Edge of chaos","State representation","Generalization"],"title":"Liquid computing on and off the edge of chaos with a striatal microcircuit","type":"publication"},{"authors":[""],"categories":null,"content":"","date":1414627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1414627200,"objectID":"f34bcbee2f448b0dbceecc241beab1de","permalink":"https://rcfduarte.github.io/talk/dondersdiscussions2014/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/dondersdiscussions2014/","section":"talk","summary":"Ecologically relevant computations are carried out by a complex interaction of adaptive dynamics, through a variety of activity-dependent modifications of synaptic and intrinsic neuronal properties. Such modifications ought to be robust and reliable enough to endow neuro- nal circuits with the ability to learn from and operate upon complex, dynamic environmental variables. On the lower levels of the cortical processing hierar- chy, continuous data streams representing the environ- ment are parsed, in order to isolate and attend to salient and invariant features (’perceptual objects’), upon which higher order cortical networks will operate, by flexibly evaluating the dynamic relations between such structural elements. The formation of stable representations of spatial/spectral environmental features (stimulus selec- tivity) along with the related ability to discriminate such features and their combinations is known to be continu- ously shaped and refined by synaptic plasticity mechan- isms, and it has been recently demonstrated that correlation-based inhibitory plasticity has an important role to play in such computations. However, in order to adequately process information, neural circuits must not only develop stable internal representations of perceptual objects, but also reflect and represent the continuous unfolding structure of its input, which is poised with intricate temporal dependen- cies. Much less is currently known about the acquisition of complex temporal relations between stimuli and the (possibly specialized) role played by different adaptation mechanisms involved in this process. In this work, we study the properties of biologically realistic networks of LIF neurons, with differentially modulated, dynamic excitation and inhibition, combining well established as well as more recent phenomenological models of synaptic plasticity. Explicitly embedded or entirely self-organized, input-specific neuronal assemblies are driven by stimulus sequences that contain complex temporal dependencies and signal propagation through- out these assemblies is gated by transient disruptions of E/I balance, in order to ‘prime’ the network to learn the underlying transitional probabilities and input statistics through targeted modifications of these ‘gating’ synapses. We explore the representational properties developed by these networks and the impact of the different plasticity rules in shaping the network’s learning abilities while maintaining stable global dynamics. Furthermore, we assess the network’s ability to extract complex temporal dependency rules between sequence elements and to use the acquired knowledge to make predictions about upcoming sequence elements.","tags":[],"title":"Temporal sequence learning via adaptation in biologically plausible spiking neural networks","type":"talk"},{"authors":["Renato Duarte","Abigail Morrison"],"categories":null,"content":"","date":1413936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1413936000,"objectID":"c7b534073dfe0e58dc4d23e15ef1544e","permalink":"https://rcfduarte.github.io/publication/dynamicstability_frontiers2014/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/dynamicstability_frontiers2014/","section":"publication","summary":"The ability to acquire and maintain appropriate representations of time-varying, sequential stimulus events is a fundamental feature of neocortical circuits and a necessary first step toward more specialized information processing. The dynamical properties of such representations depend on the current state of the circuit, which is determined primarily by the ongoing, internally generated activity, setting the ground state from which input-specific transformations emerge. Here, we begin by demonstrating that timing-dependent synaptic plasticity mechanisms have an important role to play in the active maintenance of an ongoing dynamics characterized by asynchronous and irregular firing, closely resembling cortical activity in vivo. Incoming stimuli, acting as perturbations of the local balance of excitation and inhibition, require fast adaptive responses to prevent the development of unstable activity regimes, such as those characterized by a high degree of population-wide synchrony. We establish a link between such pathological network activity, which is circumvented by the action of plasticity, and a reduced computational capacity. Additionally, we demonstrate that the action of plasticity shapes and stabilizes the transient network states exhibited in the presence of sequentially presented stimulus events, allowing the development of adequate and discernible stimulus representations. The main feature responsible for the increased discriminability of stimulus-driven population responses in plastic networks is shown to be the decorrelating action of inhibitory plasticity and the consequent maintenance of the asynchronous irregular dynamic regime both for ongoing activity and stimulus-driven responses, whereas excitatory plasticity is shown to play only a marginal role.","tags":["Transient Dynamics","Asynchronous irregular state","Excitation-inhibition balance","Online computation","Stimulus representation","Synaptic plasticity"],"title":"Dynamic stability of sequential stimulus representations in adapting neuronal networks","type":"publication"},{"authors":[""],"categories":null,"content":"","date":1409616000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409616000,"objectID":"47167276e2aaeb4e13434de8aab7bfaa","permalink":"https://rcfduarte.github.io/talk/bernsteinconference_2014/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/bernsteinconference_2014/","section":"talk","summary":"Ecologically relevant computations are carried out by a complex interaction of adaptive dynamics, through a variety of activity-dependent modifications of synaptic and intrinsic neuronal properties. Such modifications ought to be robust and reliable enough to endow neuro- nal circuits with the ability to learn from and operate upon complex, dynamic environmental variables. On the lower levels of the cortical processing hierar- chy, continuous data streams representing the environ- ment are parsed, in order to isolate and attend to salient and invariant features (’perceptual objects’), upon which higher order cortical networks will operate, by flexibly evaluating the dynamic relations between such structural elements. The formation of stable representations of spatial/spectral environmental features (stimulus selec- tivity) along with the related ability to discriminate such features and their combinations is known to be continu- ously shaped and refined by synaptic plasticity mechan- isms, and it has been recently demonstrated that correlation-based inhibitory plasticity has an important role to play in such computations. However, in order to adequately process information, neural circuits must not only develop stable internal representations of perceptual objects, but also reflect and represent the continuous unfolding structure of its input, which is poised with intricate temporal dependen- cies. Much less is currently known about the acquisition of complex temporal relations between stimuli and the (possibly specialized) role played by different adaptation mechanisms involved in this process. In this work, we study the properties of biologically realistic networks of LIF neurons, with differentially modulated, dynamic excitation and inhibition, combining well established as well as more recent phenomenological models of synaptic plasticity. Explicitly embedded or entirely self-organized, input-specific neuronal assemblies are driven by stimulus sequences that contain complex temporal dependencies and signal propagation through- out these assemblies is gated by transient disruptions of E/I balance, in order to ‘prime’ the network to learn the underlying transitional probabilities and input statistics through targeted modifications of these ‘gating’ synapses. We explore the representational properties developed by these networks and the impact of the different plasticity rules in shaping the network’s learning abilities while maintaining stable global dynamics. Furthermore, we assess the network’s ability to extract complex temporal dependency rules between sequence elements and to use the acquired knowledge to make predictions about upcoming sequence elements.","tags":[],"title":"Temporal sequence learning via adaptation in biologically plausible spiking neural networks","type":"talk"},{"authors":[""],"categories":null,"content":"","date":1406505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1406505600,"objectID":"e79beb08bda922a78758d8ce9d037c32","permalink":"https://rcfduarte.github.io/talk/cns2014_poster/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/cns2014_poster/","section":"talk","summary":"Ecologically relevant computations are carried out by a complex interaction of adaptive dynamics, through a variety of activity-dependent modifications of synaptic and intrinsic neuronal properties. Such modifications ought to be robust and reliable enough to endow neuro- nal circuits with the ability to learn from and operate upon complex, dynamic environmental variables. On the lower levels of the cortical processing hierar- chy, continuous data streams representing the environ- ment are parsed, in order to isolate and attend to salient and invariant features (’perceptual objects’), upon which higher order cortical networks will operate, by flexibly evaluating the dynamic relations between such structural elements. The formation of stable representations of spatial/spectral environmental features (stimulus selec- tivity) along with the related ability to discriminate such features and their combinations is known to be continu- ously shaped and refined by synaptic plasticity mechan- isms, and it has been recently demonstrated that correlation-based inhibitory plasticity has an important role to play in such computations. However, in order to adequately process information, neural circuits must not only develop stable internal representations of perceptual objects, but also reflect and represent the continuous unfolding structure of its input, which is poised with intricate temporal dependen- cies. Much less is currently known about the acquisition of complex temporal relations between stimuli and the (possibly specialized) role played by different adaptation mechanisms involved in this process. In this work, we study the properties of biologically realistic networks of LIF neurons, with differentially modulated, dynamic excitation and inhibition, combining well established as well as more recent phenomenological models of synaptic plasticity. Explicitly embedded or entirely self-organized, input-specific neuronal assemblies are driven by stimulus sequences that contain complex temporal dependencies and signal propagation through- out these assemblies is gated by transient disruptions of E/I balance, in order to ‘prime’ the network to learn the underlying transitional probabilities and input statistics through targeted modifications of these ‘gating’ synapses. We explore the representational properties developed by these networks and the impact of the different plasticity rules in shaping the network’s learning abilities while maintaining stable global dynamics. Furthermore, we assess the network’s ability to extract complex temporal dependency rules between sequence elements and to use the acquired knowledge to make predictions about upcoming sequence elements.","tags":[],"title":"Temporal sequence learning via adaptation in biologically plausible spiking neural networks","type":"talk"},{"authors":[""],"categories":null,"content":"","date":1406282400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1406282400,"objectID":"dc70d5263f3edcf446eff17a0d11ebb6","permalink":"https://rcfduarte.github.io/talk/agl_2014/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/agl_2014/","section":"talk","summary":"Artificial Grammar Learning (AGL) paradigms study the nature of syntactic processing and implicit sequence learning. Humans acquire complex structural dependencies by mere exposure. We investigate to which extent generic cortical circuits (excitable reservoirs of spiking neurons) can support formally explicit symbolic computations and provide qualitative comparisons with Human behavioral performance.","tags":[],"title":"Self-Organized Artificial Grammar Learning in Spiking Neural Networks","type":"talk"},{"authors":["Renato Duarte","Peggy Seriès","Abigail Morrison"],"categories":null,"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"65011be979f6b310d1bb2e955769e115","permalink":"https://rcfduarte.github.io/publication/agl_2014/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/agl_2014/","section":"publication","summary":"Through self-organization, mediated by simple biologically plausible unsupervised plasticity mechanisms, RNNs composed of binary, threshold units are capable of acquiring complex sequential structure and developing a reliable predictive model. We demonstrate that this is achieved primarily by adaptive inhibitory synapses and homeostasis whose actions enforce compactness of input-driven population responses.","tags":null,"title":"Self-Organized Artificial Grammar Learning in Spiking Neural Networks","type":"publication"},{"authors":[""],"categories":null,"content":"","date":1371513600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1371513600,"objectID":"f19422671f5148d8c3a81668b41248fa","permalink":"https://rcfduarte.github.io/talk/cns2013/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/cns2013/","section":"talk","summary":"We adopt methods/formalisms from theoretical linguistics (developed to study rule-like compositional behavior). Artificial Grammar Learning (AGL) paradigms study the nature of syntactic processing and implicit sequence learning. Humans acquire complex structural dependencies by mere exposure. We investigate to which extent generic cortical circuits (excitable reservoirs of spiking neurons) can support formally explicit symbolic computations.","tags":[],"title":"Syntax Processing properties of generic cortical circuits","type":"talk"},{"authors":[""],"categories":null,"content":"","date":1369908000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1369908000,"objectID":"636f5887eec34d46c396b63859490545","permalink":"https://rcfduarte.github.io/talk/occam2013/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/occam2013/","section":"talk","summary":"Higher cognitive functioning is assumed to be largely representational and compositional in nature. At various processing stages, from perceptual to motor, discrete structural elements with intricate temporal dependencies are combined into increasingly complex constructs. In order to address these issues and to attempt to map these complex processes to the underlying neuronal infrastructure, we adopt ideas and formalisms developed by theoretical linguistics to study the nature of rule-like or compositional behavior. The Artificial Grammar Learning (AGL) paradigm has a long tradition as a means to study the nature of syntactic processing and implicit sequence learning. With mere exposure and without performance feedback, human beings implicitly acquire knowledge about the structural regularities implemented by complex rule systems. In this work, we investigate to which extent generic cortical microcircuits can support formally explicit symbolic computations, instantiated by formal grammars and implementing various types of local and non-adjacent dependencies between the sequence elements, thus requiring varying degrees of computational complexity and online processing memory to be adequately learned. We use concrete implementations of input-driven recurrent networks composed of noisy, spiking neurons, built according to the reservoir computing framework and dynamically shaped by a variety of synaptic and intrinsic plasticity mechanisms operating concomitantly. We show that, when shaped by plasticity, these models are capable of acquiring the structure of simple (regular) grammars. Additionally, when asked to judge string legality (in a manner similar to human subjects), the networks perform at a qualitatively comparable level.","tags":[],"title":"Syntax processing properties of generic cortical circuits","type":"talk"},{"authors":[""],"categories":null,"content":"","date":1363564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1363564800,"objectID":"979f3ebcffae785f2f3a9660890c337c","permalink":"https://rcfduarte.github.io/talk/bccn_freiburg_2013/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/bccn_freiburg_2013/","section":"talk","summary":"We adopt methods/formalisms from theoretical linguistics (developed to study rule-like compositional behavior). Artificial Grammar Learning (AGL) paradigms study the nature of syntactic processing and implicit sequence learning. Humans acquire complex structural dependencies by mere exposure. We investigate to which extent generic cortical circuits (excitable reservoirs of spiking neurons) can support formally explicit symbolic computations.","tags":[],"title":"Syntax Processing properties of generic cortical circuits","type":"talk"},{"authors":[""],"categories":null,"content":"","date":1358344800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1358344800,"objectID":"cd7a6d0d0e5d37dce3b8571f96102827","permalink":"https://rcfduarte.github.io/talk/eurospin_2013/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/eurospin_2013/","section":"talk","summary":"The ability to encode, process and represent structured sequences of perceptual information as well as the ability to finely sequence motor actions are ubiquitous features of human cognition, fundamental to a variety of common, everyday tasks. Sequential learning provides a domain-general mechanism for acquiring predictive relations between sequence elements abiding to a set of structural regularities, upon which the brain can anticipate upcoming elements. To account for the ability of neuronal circuits to process data with embedded temporal dependencies (expressed as symbolic time series), recurrent neural network (RNN) models are naturally suitable by virtue of their inherent recurrent connectivity (that allows context information to be kept in units’ activities), but also due to their biological plausibility. In this work, we explore the properties and characteristics of different recurrent network models, built according to the reservoir computing framework, involved in a series of different sequence processing tasks, designed to assess their ability to acquire and learn temporal dependencies and statistics of the input data. We assess their properties and performance as ‘predictive machines’ (relating it to the capacity to learn the set of generative rules underlying different grammars), and explore their ability to adequately capture and represent variable length temporal dependencies embedded in the input sequences. We also compare models with varying degrees of biological realism, while exploring the trade-off between abstraction and biological realism in this specific domain.","tags":[],"title":"Processing structured symbolic sequences with Recurrent Neural Networks","type":"talk"},{"authors":["Renato Duarte"],"categories":null,"content":"","date":1314835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1314835200,"objectID":"efc4015b7a7e838a7104d20892b0ab40","permalink":"https://rcfduarte.github.io/publication/msc_thesis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/msc_thesis/","section":"publication","summary":"The highly recurrent connectivity encountered in the neocortical circuitry makes recurrent neural network (RNN) models highly suitable when investigating the computational properties of biologically inspired model neurodynamics. The recent reservoir computing (RC) models, an extension of the RNN paradigm, provide a framework for state-dependent computations, where information is encoded in the form of state-space trajectories, which is similar to recent findings in neurobiology. Over the past few years, several attempts have been made to endow these network models with adaptive mechanisms, capable of mimicking the various neural plasticity mechanisms known to exist in the brain and to play a fundamental role in shaping the dynamics and information processing capabilities of the underlying neural networks. In this thesis, we analyze the dynamic properties of a simple reservoir computer model, with self-organizing plasticity mechanisms operating concomitantly. We investigate how different combinations of three forms of biologically inspired adaptive mechanisms shape the reservoir’s dynamic properties and their effectiveness in acquiring an internal representation of structured symbol sequences. We demonstrate, replicating previous work, that only combined do these mechanisms allow the dynamic reservoir networks to achieve an input separation that outperforms static (i.e. without plasticity) reservoir networks. We further assess how the symbol sequences are internally represented in different network settings. All reservoir networks are shown to reflect the input structure in their state dynamics, but plasticity is clearly beneficial by modifying network parameters, increasing the network’s ability to learn the temporal structure of the input sequences.","tags":["Recurrent Neural Networks","Reservoir Computing","Plasticity","Self-organization","Sequence processing"],"title":"Self-organized sequence processing in recurrent neural networks with multiple interacting plasticity mechanisms","type":"publication"},{"authors":[""],"categories":null,"content":"","date":1314576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1314576000,"objectID":"80ccc7267f1a755175e755b38a2af6c6","permalink":"https://rcfduarte.github.io/talk/msc_thesis_defence/","publishdate":"2018-05-05T00:00:00Z","relpermalink":"/talk/msc_thesis_defence/","section":"talk","summary":"","tags":[],"title":"Self-organized sequence processing in Recurrent Neural Networks with multiple interacting plasticity mechanisms","type":"talk"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://rcfduarte.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://rcfduarte.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]